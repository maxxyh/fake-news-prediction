{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following this tutorial: https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "# TODO: This tutorial has enhancements: https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A little less than a decade ago, hockey fans w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The writers of the HBO series The Sopranos too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Despite claims from the TV news outlet to offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>After receiving 'subpar' service and experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>After watching his beloved Seattle Mariners pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                               text\n",
       "0       1  A little less than a decade ago, hockey fans w...\n",
       "1       1  The writers of the HBO series The Sopranos too...\n",
       "2       1  Despite claims from the TV news outlet to offe...\n",
       "3       1  After receiving 'subpar' service and experienc...\n",
       "4       1  After watching his beloved Seattle Mariners pr..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "df = pd.read_csv(\"raw_data/fulltrain.csv\", names=[\"labels\", \"text\"])\n",
    "test_df = pd.read_csv(\"raw_data/balancedtest.csv\", names=[\"labels\", \"text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = str(text).replace(r'http[\\w:/\\.]+','') # removing urls\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "    \n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "test_df[\"text\"] = test_df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x and y\n",
    "X = df[\"text\"]\n",
    "y = df[\"labels\"]\n",
    "\n",
    "# test x and y\n",
    "X_test = test_df['text']\n",
    "y_test = test_df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tags = [\"trusted\", \"satire\", \"hoax\", \"propaganda\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION SET\n",
      "accuracy 0.6306884082690865\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     trusted       0.85      0.65      0.74      4244\n",
      "      satire       0.99      0.09      0.16      2065\n",
      "        hoax       0.52      1.00      0.68      5313\n",
      "  propaganda       1.00      0.34      0.50      3035\n",
      "\n",
      "    accuracy                           0.63     14657\n",
      "   macro avg       0.84      0.52      0.52     14657\n",
      "weighted avg       0.78      0.63      0.59     14657\n",
      "\n",
      "TEST SET\n",
      "accuracy 0.362\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     trusted       0.64      0.29      0.40       750\n",
      "      satire       0.75      0.00      0.01       750\n",
      "        hoax       0.29      1.00      0.46       750\n",
      "  propaganda       1.00      0.16      0.27       750\n",
      "\n",
      "    accuracy                           0.36      3000\n",
      "   macro avg       0.67      0.36      0.28      3000\n",
      "weighted avg       0.67      0.36      0.28      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"VALIDATION SET\")\n",
    "y_pred = nb.predict(X_val)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "print(classification_report(y_val, y_pred,target_names=my_tags))\n",
    "\n",
    "\n",
    "print(\"TEST SET\")\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred, target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION SET\n",
      "accuracy 0.8894043801596507\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     trusted       0.82      0.95      0.88      4244\n",
      "      satire       0.95      0.86      0.90      2065\n",
      "        hoax       0.89      0.95      0.92      5313\n",
      "  propaganda       0.97      0.71      0.82      3035\n",
      "\n",
      "    accuracy                           0.89     14657\n",
      "   macro avg       0.91      0.87      0.88     14657\n",
      "weighted avg       0.90      0.89      0.89     14657\n",
      "\n",
      "TEST SET\n",
      "accuracy 0.6766666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     trusted       0.70      0.77      0.73       750\n",
      "      satire       0.72      0.35      0.47       750\n",
      "        hoax       0.55      0.95      0.70       750\n",
      "  propaganda       0.90      0.65      0.75       750\n",
      "\n",
      "    accuracy                           0.68      3000\n",
      "   macro avg       0.72      0.68      0.66      3000\n",
      "weighted avg       0.72      0.68      0.66      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "print(\"VALIDATION SET\")\n",
    "y_pred = sgd.predict(X_val)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "print(classification_report(y_val, y_pred,target_names=my_tags))\n",
    "\n",
    "\n",
    "print(\"TEST SET\")\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION SET\n",
      "accuracy 0.9502626731254691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     trusted       0.93      0.97      0.95      4244\n",
      "      satire       0.97      0.92      0.95      2065\n",
      "        hoax       0.95      0.98      0.96      5313\n",
      "  propaganda       0.97      0.90      0.93      3035\n",
      "\n",
      "    accuracy                           0.95     14657\n",
      "   macro avg       0.95      0.94      0.95     14657\n",
      "weighted avg       0.95      0.95      0.95     14657\n",
      "\n",
      "TEST SET\n",
      "accuracy 0.7383333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     trusted       0.81      0.80      0.81       750\n",
      "      satire       0.82      0.41      0.54       750\n",
      "        hoax       0.60      0.88      0.71       750\n",
      "  propaganda       0.83      0.87      0.85       750\n",
      "\n",
      "    accuracy                           0.74      3000\n",
      "   macro avg       0.76      0.74      0.73      3000\n",
      "weighted avg       0.76      0.74      0.73      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(random_state=0, max_iter=10000)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "print(\"VALIDATION SET\")\n",
    "y_pred = logreg.predict(X_val)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "print(classification_report(y_val, y_pred,target_names=my_tags))\n",
    "\n",
    "\n",
    "print(\"TEST SET\")\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Word2Vec with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/68/c2vd8ndn12s0ky3brj493wzr0000gn/T/ipykernel_20317/2829345770.py:4: DeprecationWarning: Call to deprecated `init_sims` (Use fill_norms() instead. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  wv.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load word vectors \n",
    "from gensim.models import Word2Vec\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"word_vec/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.key_to_index:\n",
    "            mean.append(wv.syn0norm[wv.key_to_index[word]])\n",
    "            all_words.add(wv.key_to_index[word])\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'wv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m val_tokenized \u001b[39m=\u001b[39m val\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m r: w2v_tokenize_text(r[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     13\u001b[0m train_tokenized \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m r: w2v_tokenize_text(r[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mvalues\n\u001b[0;32m---> 15\u001b[0m X_train_word_average \u001b[39m=\u001b[39m word_averaging_list(wv,train_tokenized)\n\u001b[1;32m     16\u001b[0m X_val_word_average \u001b[39m=\u001b[39m word_averaging_list(wv,val_tokenized)\n",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m, in \u001b[0;36mword_averaging_list\u001b[0;34m(wv, text_list)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m  \u001b[39mword_averaging_list\u001b[39m(wv, text_list):\n\u001b[0;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mvstack([word_averaging(wv, post) \u001b[39mfor\u001b[39;00m post \u001b[39min\u001b[39;00m text_list ])\n",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m  \u001b[39mword_averaging_list\u001b[39m(wv, text_list):\n\u001b[0;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mvstack([word_averaging(wv, post) \u001b[39mfor\u001b[39;00m post \u001b[39min\u001b[39;00m text_list ])\n",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m, in \u001b[0;36mword_averaging\u001b[0;34m(wv, words)\u001b[0m\n\u001b[1;32m      6\u001b[0m         mean\u001b[39m.\u001b[39mappend(word)\n\u001b[1;32m      7\u001b[0m     \u001b[39melif\u001b[39;00m word \u001b[39min\u001b[39;00m wv\u001b[39m.\u001b[39mkey_to_index:\n\u001b[0;32m----> 8\u001b[0m         mean\u001b[39m.\u001b[39mappend(wv\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39msyn0norm[wv\u001b[39m.\u001b[39mkey_to_index[word]])\n\u001b[1;32m      9\u001b[0m         all_words\u001b[39m.\u001b[39madd(wv\u001b[39m.\u001b[39mkey_to_index[word])\n\u001b[1;32m     11\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mean:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyedVectors' object has no attribute 'wv'"
     ]
    }
   ],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "    \n",
    "train, val = train_test_split(df, test_size=0.3, random_state = 42)\n",
    "\n",
    "val_tokenized = val.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n",
    "\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_val_word_average = word_averaging_list(wv,val_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(random_state=0, max_iter=10000)\n",
    "logreg = logreg.fit(X_train_word_average, train['labels'])\n",
    "y_pred = logreg.predict(X_val_word_average)\n",
    "print('accuracy %s' % accuracy_score(y_pred, val.labels))\n",
    "print(classification_report(val.labels, y_pred,target_names=my_tags))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like it was pretty bad... in that case, we shall try..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DocVec + Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be continued..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
